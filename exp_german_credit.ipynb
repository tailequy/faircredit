{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import lib\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from my_useful_functions import calculate_performance_statistical_parity,calculate_performance_equalized_odds,calculate_performance_equal_opportunity,calculate_performance_predictive_parity,calculate_performance_predictive_equality,calculate_performance_treatment_equality\n",
    "from sklearn import preprocessing\n",
    "from aif360.datasets.binary_label_dataset import BinaryLabelDataset\n",
    "# DT\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# NB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "# MLP\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "# kNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# Agarwal\n",
    "from exponentiated_gradient_reduction import ExponentiatedGradientReduction\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import tree\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "# DIR\n",
    "from disparate_impact_remover import DisparateImpactRemover\n",
    "# LFR\n",
    "from learning_fair_representations import LFR\n",
    "# EOP\n",
    "from eq_odds_postprocessing import EqOddsPostprocessing\n",
    "# CEP\n",
    "from calibrated_eq_odds_postprocessing import CalibratedEqOddsPostprocessing\n",
    "from compute_abroca import *\n",
    "import numpy as np\n",
    "import sklearn.metrics as metrics \n",
    "from scipy import interpolate\n",
    "from scipy import integrate\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "from IPython.display import Markdown, display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "\n",
    "from sklearn import metrics\n",
    "import seaborn as sns\n",
    "import seaborn as sn\n",
    "\n",
    "\n",
    "import seaborn; seaborn.set_style('whitegrid')\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "#from aif360.datasets import StandardDataset\n",
    "#from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from pomegranate import *\n",
    "import pygraphviz\n",
    "numpy.random.seed(0)\n",
    "numpy.set_printoptions(suppress=True)\n",
    "from sklearn.model_selection import train_test_split\n",
    "import networkx\n",
    "from pomegranate.utils import plot_networkx\n",
    "#matplotlib.use('TkAgg')\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistic Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"data/german_data_credit.csv\")\n",
    "risk={1:\"Good\", 2:\"Bad\"}\n",
    "df[\"class-label\"]=df[\"class-label\"].map(risk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#statistical_parity\n",
    "def statistical_parity_difference(dataset, protected,classes,majority_group,minority_group,positive_value):\n",
    "    ratio = round(pd.crosstab(dataset[protected], dataset[classes]).div(pd.crosstab(dataset[protected], dataset[classes]).apply(sum,1),0),4)*100\n",
    "    #return ratio\n",
    "    return ratio.loc[majority_group][positive_value]-ratio.loc[minority_group][positive_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_info(dataset,class_label,positive_value,negative_value,protected_attr,majority_group,minority_group):\n",
    "    categorical_columns = (dataset.select_dtypes(['category', 'object']).columns)\n",
    "    numeric_columns = (dataset.select_dtypes(['number']).columns)\n",
    "    for attr in dataset.keys():\n",
    "        if attr in categorical_columns:    \n",
    "            print(attr ,' & Categorical & ',len(dataset[attr].value_counts()),'& &',dataset[attr].isnull().sum(),'\\\\\\\\')\n",
    "        else:           \n",
    "            print(attr,'& Numerical &',len(dataset[attr].value_counts()),'&',min(dataset[attr]),\"-\",max(dataset[attr]),'&',dataset[attr].isnull().sum(),'\\\\\\\\')  \n",
    "    \n",
    "    print('Class distribution:',dataset[class_label].value_counts())\n",
    "    print('Class imbalance:',len(dataset.loc[dataset[class_label]==negative_value])/len(dataset.loc[dataset[class_label]==positive_value]))\n",
    "    print('statistical parity:',statistical_parity_difference(dataset,protected_attr,class_label,majority_group,minority_group,positive_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_info(df,class_label=\"class-label\",positive_value=\"Good\",negative_value=\"Bad\",\n",
    "           protected_attr=\"sex\",majority_group='male',minority_group='female')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check missing values\n",
    "print('Missing values')\n",
    "for i,j in zip(df.columns,(df.values.astype(str) == '?').sum(axis = 0)):\n",
    "    if j > 0:\n",
    "        print(str(i) + ': ' + str(j) + ' records')\n",
    "print(\"---------------------------------\")\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bayesian_network(a, X, labels, name):\n",
    "  model = BayesianNetwork.from_samples(X, algorithm='exact',constraint_graph=a,state_names=labels)\n",
    "  plt.figure(figsize=(15, 12),dpi=400)\n",
    "  model.plot()\n",
    "  print(model.structure)\n",
    "  plt.savefig(name, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def german_credit():\n",
    "  df=pd.read_csv(\"data/german_data_credit.csv\")\n",
    "  print(\"Length:\",len(df))\n",
    "  print(\"Number of attribute:\",len(df.columns))\n",
    "  \n",
    "  print(df['class-label'].value_counts())\n",
    "\n",
    "  X = df.values\n",
    "  for i in range(len(list(df.keys()))):\n",
    "    if len(pd.unique(df[df.keys()[i]])) == 2:\n",
    "      print(i,list(df.keys())[i], len(pd.unique(df[df.keys()[i]]))) \n",
    "  labels=list(df.keys())\n",
    "\n",
    "  X[:,1]=np.array([0 if v<=6 else 1 if v>6 and v<=12 else 2 for v in X[:,1]]) #duration\n",
    "  X[:,4]=np.array([0 if v<=2000 else 1 if v>2000 and v<=5000 else 2 for v in X[:,4]]) #credit amount\n",
    "  X[:,11]=np.array([0 if v<25 else 1 for v in X[:,11]]) #age\n",
    " \n",
    "  a = networkx.DiGraph()\n",
    "  b = tuple((0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20))\n",
    "  c= tuple((21, ))\n",
    "  a.add_edge(b,c)\n",
    "  a.add_edge(b,b)\n",
    "\n",
    "  return a, X, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, X, labels = german_credit()\n",
    "plot_bayesian_network(a, X, labels, 'German.BN.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def german_credit_plot_1():\n",
    "  df=pd.read_csv(\"data/german_data_credit.csv\")\n",
    "  print(\"Length:\",len(df))\n",
    "  print(\"Number of attribute:\",len(df.columns))\n",
    "\n",
    "  print(len(df[(df['credit-amount'] < 5000) & (df['credit-amount'] >= 2000) & (df[\"duration\"] > 48)]))\n",
    "\n",
    "  a24 = [422, 284, 64]\n",
    "  a48 = [10, 96, 108]\n",
    "  a72 = [108, 0, 16]\n",
    "\n",
    "  \n",
    "  groups = (\"<= 2000\", \"2000 - 5000\", \"> 5000\")\n",
    "  groups_value = {\n",
    "      '0-2 years': (422, 284, 64),\n",
    "      '2-4 years': (10, 96, 108),\n",
    "      '4-6 years': (108, 0, 16),\n",
    "  }\n",
    "\n",
    "  x = np.arange(len(groups))  # the label locations\n",
    "  width = 0.2  # the width of the bars\n",
    "  multiplier = 0\n",
    "  \n",
    "\n",
    "  fig, ax = plt.subplots(layout='constrained', figsize=(12, 6))\n",
    "\n",
    "  column_sums = np.sum(list(groups_value.values()), axis=0)\n",
    "  for attribute, measurement in groups_value.items():\n",
    "      offset = width * multiplier\n",
    "      percentage = [100 * value / column_sums[index] for index, value in enumerate(measurement)]\n",
    "      # rects = ax.bar(x + offset, [round(_, 2) for _ in percentage], width, label=attribute)\n",
    "      rects = ax.bar(x + offset, measurement, width, label=attribute)\n",
    "      ax.bar_label(rects, labels=[f\"{(round(_, 2))}%\" for _ in percentage], padding=4)\n",
    "      multiplier += 1\n",
    "\n",
    "  # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "  ax.set_ylabel('Population', fontsize=14, fontweight='bold')\n",
    "  ax.set_xlabel('Credit amount', fontsize=14, fontweight='bold')\n",
    "  # ax.set_title('Penguin attributes by species')\n",
    "  ax.set_xticks(x + width, groups)\n",
    "  plt.legend(title=\"Duration\", loc='upper left', bbox_to_anchor=(0.3, 1))\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_credit_plot_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as mtick\n",
    "def plot_2_variable(dataset,var1, var2, width=0.5, wide=9,height=5,legend=\"\", xlabel=\"\", logy=False,verbose=True,indexlist=\"\",index=False):\n",
    "    \n",
    "    var1_var2_ratio=round(pd.crosstab(dataset[var1], dataset[var2]).div(pd.crosstab(dataset[var1], dataset[var2]).apply(sum,1),0),3)*100\n",
    "    var1_var2 =  pd.crosstab(dataset[var1], dataset[var2])\n",
    "    if verbose:\n",
    "        print(var1_var2)\n",
    "        \n",
    "    if index:\n",
    "        var1_var2 = var1_var2.reindex(indexlist)\n",
    "        var1_var2_ratio = var1_var2_ratio.reindex(indexlist)\n",
    "    \n",
    "    ax = var1_var2.plot(kind='bar',width=width,figsize=(wide,height)\n",
    "                     ,logy=logy\n",
    "                     )\n",
    "    #x = hour_income_gender.plot(kind ='bar',stacked=False)\n",
    "    xlocs, xlabs = plt.xticks()\n",
    "    for i in range(len(var1_var2)):\n",
    "        for j in range(len(var1_var2.iloc[i])):\n",
    "            plt.text(xlocs[i]-0.35+j*0.3, var1_var2.iloc[i][j] + 2,str(round(var1_var2_ratio.iloc[i][j],1))+\"%\" )\n",
    "    #plt.title('Distribution of '+ title+ ' across '+ legend)\n",
    "    plt.ylabel('Population',fontsize=14,fontweight='bold')\n",
    "    plt.xlabel(xlabel,fontsize=14,fontweight='bold')\n",
    "    \n",
    "    ax.yaxis.set_major_formatter(mtick.StrMethodFormatter('{x:,.0f}'))\n",
    "    plt.rc('xtick',labelsize=13)\n",
    "    plt.rc('ytick',labelsize=13)\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.legend(title=legend,prop={'size': 12})\n",
    "    filename=var1+\"_\"+var2+\".pdf\"\n",
    "    plt.savefig(filename,bbox_inches='tight')    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2_variable(df,var1='checking-account', var2='class-label', width=0.8, wide=8,height=4,\n",
    "                legend=\"Class\", xlabel=\"Status of checking account\", logy=False,verbose=True,indexlist=[\"no account\", \"<0 DM\",\"0 <= <200 DM\",\">= 200 DM \"],index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Credit amount\n",
    "sns.distplot(df['credit-amount'], hist=True, kde=False, \n",
    "             bins=40, color = 'darkblue', \n",
    "             hist_kws={'edgecolor':'black'},\n",
    "             kde_kws={'linewidth': 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Duration\n",
    "sns.distplot(df['duration'], hist=True, kde=False, \n",
    "             bins=10, color = 'darkblue', \n",
    "             hist_kws={'edgecolor':'black'},\n",
    "             kde_kws={'linewidth': 4})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## German Credit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_german_credit():\n",
    "    df = pd.read_csv('data/german_data_credit.csv')    \n",
    "    protected_attribute = 'sex'\n",
    "    majority_group_name = \"male\"\n",
    "    minority_group_name = \"female\"\n",
    "    class_label = 'class-label' \n",
    "    \n",
    "    print(\"Length:\",len(df))\n",
    "    print(\"Number of attribute:\",len(df.columns))\n",
    "    \n",
    "    print(\"Length (cleaned):\",len(df))\n",
    "    print(\"Class imbalance: \\n\",df[class_label].value_counts())\n",
    "    #label encode\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    for i in df.columns:\n",
    "        if df[i].dtypes == 'object':\n",
    "            df[i] = le.fit_transform(df[i])\n",
    "    #Splitting data into train and test\n",
    "    length = len(df.columns)\n",
    "    X = df.iloc[:,:length-1]\n",
    "    y = df[class_label]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) \n",
    "    \n",
    "    #Get index    \n",
    "    feature = X.keys().tolist()    \n",
    "    sa_index = feature.index(protected_attribute)\n",
    "    p_Group = 0 \n",
    "    \n",
    "    return X_train, X_test, y_train, y_test,sa_index, p_Group, protected_attribute,majority_group_name,minority_group_name"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(dataset, X_train, X_test, y_train, y_test,sa_index, p_Group,protected_attribute,majority_group_name,minority_group_name, algorithm, preprocessing_algorithm='', postprocessing_algorithm=''):    \n",
    "    datasetTrain = BinaryLabelDataset(df=pd.concat([X_train, y_train.to_frame()], axis=1), label_names=[y_train.name], protected_attribute_names=[protected_attribute])\n",
    "    datasetTest = BinaryLabelDataset(df=pd.concat([X_test, y_test.to_frame()], axis=1), label_names=[y_test.name], protected_attribute_names=[protected_attribute])\n",
    "    \n",
    "    # preprocessing\n",
    "    if preprocessing_algorithm != '': \n",
    "        if preprocessing_algorithm == 'DIR':\n",
    "            pre_model = DisparateImpactRemover(sensitive_attribute=protected_attribute)\n",
    "        elif preprocessing_algorithm == 'LFR':\n",
    "            privileged_groups = [{protected_attribute: 1.0}]\n",
    "            unprivileged_groups = [{protected_attribute: 0.0}]\n",
    "            pre_model = LFR(unprivileged_groups=unprivileged_groups,privileged_groups=privileged_groups)\n",
    "        dataset_train_transf = pre_model.fit_transform(datasetTrain)\n",
    "        dataset_test_transf = pre_model.transform(datasetTest)\n",
    "\n",
    "        X_train_transf = dataset_train_transf.features\n",
    "        y_train_transf = dataset_train_transf.labels.ravel()\n",
    "        X_test_transf = dataset_test_transf.features\n",
    "        y_test_transf= dataset_test_transf.labels.ravel()\n",
    "\n",
    "        X_train_transf = pd.DataFrame(X_train_transf, columns = X_train.columns)\n",
    "        y_train_transf = pd.Series(y_train_transf, name = y_train.name).astype(int)\n",
    "        X_test_transf = pd.DataFrame(X_test_transf, columns = X_train.columns)\n",
    "        y_test_transf = pd.Series(y_test_transf, name = y_train.name).astype(int)\n",
    "    \n",
    "    # inprocessing\n",
    "    if algorithm == 'DT':\n",
    "        model = tree.DecisionTreeClassifier(random_state=0)  \n",
    "    elif algorithm == 'NB': \n",
    "        model = GaussianNB()\n",
    "    elif algorithm == 'MLP':\n",
    "        model = MLPClassifier(random_state=1, max_iter=300)\n",
    "    elif algorithm == 'kNN':\n",
    "        model = KNeighborsClassifier(n_neighbors=5)\n",
    "    elif algorithm == 'Agar':\n",
    "        clf =  NB = GaussianNB()\n",
    "        model = ExponentiatedGradientReduction(prot_attr=protected_attribute,estimator=clf, constraints = \"EqualizedOdds\")\n",
    "    \n",
    "    if preprocessing_algorithm != '':\n",
    "        model.fit(X_train_transf, y_train_transf)\n",
    "        y_predicts = model.predict(X_test_transf)\n",
    "    else:\n",
    "        model.fit(X_train,y_train)\n",
    "        y_predicts = model.predict(X_test)\n",
    "\n",
    "    # postprocessing\n",
    "    if postprocessing_algorithm != '':\n",
    "        privileged_groups = [{protected_attribute: 1.0}]\n",
    "        unprivileged_groups = [{protected_attribute: 0.0}]\n",
    "        if postprocessing_algorithm == 'EOP':\n",
    "            post_model = EqOddsPostprocessing(unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups, seed=42)\n",
    "        if postprocessing_algorithm == 'CEP':\n",
    "            post_model = CalibratedEqOddsPostprocessing(unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups, seed=42)\n",
    "        \n",
    "        y_test_predicts = model.predict(X_test)\n",
    "        y_train_predicts = model.predict(X_train)\n",
    "\n",
    "        X_train_predicts = X_train.copy()\n",
    "        X_test_predicts = X_test.copy()\n",
    "\n",
    "        X_train_predicts[y_train.name] = y_train_predicts\n",
    "        X_test_predicts[y_train.name] = y_test_predicts\n",
    "    \n",
    "        dataset_train_true = BinaryLabelDataset(df=pd.concat([X_train, y_train.to_frame()], axis=1), label_names=[y_train.name], protected_attribute_names=[protected_attribute])\n",
    "        dataset_train_predicts = BinaryLabelDataset(df=X_train_predicts, label_names=[y_train.name], protected_attribute_names=[protected_attribute])\n",
    "        dataset_test_predicts = BinaryLabelDataset(df=X_test_predicts, label_names=[y_test.name], protected_attribute_names=[protected_attribute])\n",
    "\n",
    "        post_model.fit_predict(dataset_true=dataset_train_true, dataset_pred=dataset_train_predicts)\n",
    "        dataset_predicts_transf = post_model.predict(dataset_test_predicts)\n",
    "        data_predicts = dataset_predicts_transf.convert_to_dataframe()[0]\n",
    "        y_predicts = data_predicts[y_test.name].astype(int)\n",
    "\n",
    "    print(\"Statistical parity:\")\n",
    "    print(calculate_performance_statistical_parity(X_test.values, y_test.values, y_predicts, sa_index, p_Group))\n",
    "         \n",
    "    print(\"Equal opportunity\")\n",
    "    print(calculate_performance_equal_opportunity(X_test.values, y_test.values, y_predicts,  sa_index, p_Group))\n",
    "        \n",
    "    print(\"Equalized odds\")\n",
    "    print(calculate_performance_equalized_odds(X_test.values, y_test.values, y_predicts, sa_index, p_Group))\n",
    "         \n",
    "    print(\"Predictive parity\")\n",
    "    print(calculate_performance_predictive_parity(X_test.values, y_test.values, y_predicts,  sa_index, p_Group))\n",
    "        \n",
    "    print(\"Predictive equality\")\n",
    "    print(calculate_performance_predictive_equality(X_test.values, y_test.values, y_predicts,  sa_index, p_Group))\n",
    "        \n",
    "    print(\"Treatment equality\")\n",
    "    print(calculate_performance_treatment_equality(X_test.values, y_test.values, y_predicts,  sa_index, p_Group))\n",
    "        \n",
    "    filename = '{}.{}.abroca.pdf'.format(dataset, preprocessing_algorithm+algorithm)\n",
    "    #make predictions\n",
    "    if postprocessing_algorithm == '':\n",
    "        if preprocessing_algorithm:\n",
    "            X_test['pred_proba'] = model.predict_proba(X_test_transf)[:,1:2]\n",
    "        else:\n",
    "            X_test['pred_proba'] = model.predict_proba(X_test)[:,1:2]\n",
    "        X_test['true_label'] = y_test\n",
    "        df_test = X_test\n",
    "\n",
    "        #Compute Abroca\n",
    "        slice = compute_abroca(df_test, pred_col = 'pred_proba' , label_col = 'true_label', protected_attr_col = protected_attribute,\n",
    "                            majority_protected_attr_val = 1, n_grid = 10000,\n",
    "                            plot_slices = True, majority_group_name=majority_group_name ,minority_group_name=minority_group_name,file_name = filename)\n",
    "        print(\"ABROCA:\",slice)\n",
    "        plt.clf() \n",
    "    plt.clf() \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main function\n",
    "def run_eval(dataset, algorithm, preprocessing_algorithm='', postprocessing_algorithm=''):\n",
    "    if dataset == 'credit-approval':\n",
    "        X_train, X_test, y_train, y_test,sa_index, p_Group,protected_attribute,majority_group_name,minority_group_name = load_credit_approval()\n",
    "        run_experiment(dataset, X_train, X_test, y_train, y_test,sa_index, p_Group,protected_attribute,majority_group_name,minority_group_name, algorithm, preprocessing_algorithm, postprocessing_algorithm)                                        \n",
    "    if dataset == 'credit-card':\n",
    "        X_train, X_test, y_train, y_test,sa_index, p_Group,protected_attribute,majority_group_name,minority_group_name = load_credit_card()\n",
    "        run_experiment(dataset, X_train, X_test, y_train, y_test,sa_index, p_Group,protected_attribute,majority_group_name,minority_group_name, algorithm, preprocessing_algorithm, postprocessing_algorithm)                                        \n",
    "    if dataset == 'german-credit':\n",
    "        X_train, X_test, y_train, y_test,sa_index, p_Group,protected_attribute,majority_group_name,minority_group_name = load_german_credit()\n",
    "        run_experiment(dataset, X_train, X_test, y_train, y_test,sa_index, p_Group,protected_attribute,majority_group_name,minority_group_name, algorithm, preprocessing_algorithm, postprocessing_algorithm)                                            \n",
    "    if dataset == 'PAKDD':\n",
    "        X_train, X_test, y_train, y_test,sa_index, p_Group,protected_attribute,majority_group_name,minority_group_name = load_PAKDD2010()\n",
    "        run_experiment(dataset, X_train, X_test, y_train, y_test,sa_index, p_Group,protected_attribute,majority_group_name,minority_group_name, algorithm, preprocessing_algorithm, postprocessing_algorithm)                                                                \n",
    "    if dataset == 'credit-scoring':\n",
    "        X_train, X_test, y_train, y_test,sa_index, p_Group,protected_attribute,majority_group_name,minority_group_name = load_credit_scoring()\n",
    "        run_experiment(dataset, X_train, X_test, y_train, y_test,sa_index, p_Group,protected_attribute,majority_group_name,minority_group_name, algorithm, preprocessing_algorithm, postprocessing_algorithm)                                                                        \n",
    "    if dataset == 'application':\n",
    "        X_train, X_test, y_train, y_test,sa_index, p_Group,protected_attribute,majority_group_name,minority_group_name = load_application()\n",
    "        run_experiment(dataset, X_train, X_test, y_train, y_test,sa_index, p_Group,protected_attribute,majority_group_name,minority_group_name, algorithm, preprocessing_algorithm, postprocessing_algorithm)                                                                       "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## German Credit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_eval(dataset='german-credit', algorithm='DT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_eval(dataset='german-credit', algorithm='NB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_eval(dataset='german-credit', algorithm='MLP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_eval(dataset='german-credit', algorithm='kNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_eval(dataset='german-credit', algorithm='Agar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_eval(dataset='german-credit', algorithm='DT', preprocessing_algorithm='DIR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_eval(dataset='german-credit', algorithm='NB', preprocessing_algorithm='DIR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_eval(dataset='german-credit', algorithm='MLP', preprocessing_algorithm='DIR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_eval(dataset='german-credit', algorithm='kNN', preprocessing_algorithm='DIR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_eval(dataset='german-credit', algorithm='DT', preprocessing_algorithm='LFR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_eval(dataset='german-credit', algorithm='NB', preprocessing_algorithm='LFR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_eval(dataset='german-credit', algorithm='MLP', preprocessing_algorithm='LFR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_eval(dataset='german-credit', algorithm='kNN', preprocessing_algorithm='LFR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_eval(dataset='german-credit', algorithm='DT', postprocessing_algorithm='EOP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_eval(dataset='german-credit', algorithm='NB', postprocessing_algorithm='EOP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_eval(dataset='german-credit', algorithm='MLP', postprocessing_algorithm='EOP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_eval(dataset='german-credit', algorithm='kNN', postprocessing_algorithm='EOP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_eval(dataset='german-credit', algorithm='DT', postprocessing_algorithm='CEP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_eval(dataset='german-credit', algorithm='NB', postprocessing_algorithm='CEP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_eval(dataset='german-credit', algorithm='MLP', postprocessing_algorithm='CEP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_eval(dataset='german-credit', algorithm='kNN', postprocessing_algorithm='CEP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
